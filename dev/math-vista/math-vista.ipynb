{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a6ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d51078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd70e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "splits = {\n",
    "    \"testmini\": \"data/testmini-00000-of-00001-725687bf7a18d64b.parquet\",\n",
    "    \"test\": \"data/test-*.parquet\",\n",
    "}\n",
    "df = pl.read_parquet(\"hf://datasets/AI4Math/MathVista/\" + splits[\"testmini\"]).sample(\n",
    "    fraction=1.0, shuffle=True, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e02b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, TypedDict, cast\n",
    "\n",
    "\n",
    "class DecodedImage(TypedDict):\n",
    "    bytes: bytes\n",
    "\n",
    "\n",
    "class Scenario(TypedDict):\n",
    "    pid: int\n",
    "    question: str\n",
    "    answer: str\n",
    "    image: str\n",
    "    decoded_image: DecodedImage\n",
    "\n",
    "\n",
    "val_scenarios = cast(list[Scenario], df.head(64).to_dicts())\n",
    "train_scenarios_iter = cast(Iterator[Scenario], df.tail(-64).iter_rows(named=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9287d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhilton\u001b[0m (\u001b[33mwandb\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sky/sky_workdir/dev/math-vista/wandb/run-20251015_034748-001</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/wandb/math-vista/runs/001' target=\"_blank\">001</a></strong> to <a href='https://wandb.ai/wandb/math-vista' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wandb/math-vista' target=\"_blank\">https://wandb.ai/wandb/math-vista</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wandb/math-vista/runs/001' target=\"_blank\">https://wandb.ai/wandb/math-vista/runs/001</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 03:47:52 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore # noqa: F401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-15 03:47:57 [__init__.py:235] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.6: Fast Qwen2_5_Vl patching. Transformers: 4.53.2. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.811 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "Unsloth: Vision model detected, setting approx_max_num_seqs to 1\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 139.81 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 1.\n",
      "Unsloth: vLLM's KV Cache can use up to 104.11 GB. Also swap space = 6 GB.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 10-15 03:48:11 [config.py:1604] Using max model len 32768\n",
      "WARNING 10-15 03:48:11 [arg_utils.py:1511] --enable-prefix-caching is not supported for multimodal models in V0 and has been disabled.\n",
      "INFO 10-15 03:48:11 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=32768.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['embed_tokens', 'embedding', 'lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'router', 'visual', 'model.visual.blocks.31.mlp', 'model.visual.blocks.24.attn', 'model.visual.blocks.30.mlp', 'model.visual.blocks.30.attn', 'model.visual.blocks.25.attn', 'model.visual.blocks.29.attn', 'model.visual.blocks.26.attn', 'model.visual.blocks.28.attn', 'model.visual.blocks.19.attn', 'model.visual.blocks.29.mlp', 'model.visual.blocks.28.mlp', 'model.visual.blocks.31.attn', 'model.visual.blocks.25.mlp', 'model.visual.blocks.26.mlp', 'model.visual.blocks.20.attn', 'model.visual.blocks.27.mlp', 'model.visual.blocks.17.attn', 'model.visual.blocks.24.mlp', 'model.visual.blocks.18.attn', 'model.visual.blocks.16.attn', 'model.visual.blocks.11.attn', 'model.visual.blocks.21.mlp', 'model.visual.blocks.20.mlp', 'model.visual.blocks.23.mlp', 'model.visual.blocks.9.attn', 'model.visual.blocks.12.attn', 'model.visual.blocks.23.attn', 'model.visual.blocks.19.mlp', 'model.visual.blocks.22.mlp', 'model.visual.blocks.18.mlp', 'model.visual.blocks.13.attn', 'model.visual.blocks.8.attn', 'model.visual.blocks.11.mlp', 'model.visual.blocks.10.mlp', 'model.visual.blocks.6.attn', 'model.visual.blocks.15.mlp', 'model.visual.blocks.8.mlp', 'model.visual.blocks.9.mlp', 'model.visual.blocks.14.attn', 'model.visual.blocks.5.mlp', 'model.visual.blocks.14.mlp', 'model.visual.blocks.10.attn', 'model.visual.blocks.6.mlp', 'model.visual.blocks.7.mlp', 'model.visual.blocks.5.attn', 'model.visual.blocks.4.mlp', 'model.visual.blocks.16.mlp', 'model.visual.blocks.12.mlp', 'model.visual.blocks.13.mlp', 'model.visual.blocks.2.mlp', 'model.visual.blocks.3.mlp', 'model.visual.blocks.1.attn', 'model.visual.blocks.0.attn', 'model.visual.blocks.4.attn', 'model.visual.blocks.2.attn', 'model.visual.blocks.15.attn', 'model.visual.blocks.3.attn', 'model.visual.blocks.1.mlp', 'model.visual.blocks.17.mlp', 'model.visual.blocks.0.mlp', 'model.visual.blocks.7.attn', 'model.visual.blocks.31.mlp.down_proj'], 'llm_int8_threshold': 6.0}\n",
      "INFO 10-15 03:48:11 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":1,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 10-15 03:48:12 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 10-15 03:48:12 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-15 03:48:12 [model_runner.py:1083] Starting to load model unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit...\n",
      "WARNING 10-15 03:48:13 [vision.py:91] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 10-15 03:48:13 [bitsandbytes_loader.py:733] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 10-15 03:48:13 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 10-15 03:48:14 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 32.91it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.04it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 03:48:15 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 10-15 03:48:15 [model_runner.py:1115] Model loading took 6.5306 GiB and 2.247172 seconds\n",
      "INFO 10-15 03:48:24 [worker.py:295] Memory profiling takes 7.94 seconds\n",
      "INFO 10-15 03:48:24 [worker.py:295] the current vLLM instance can use total_gpu_memory (139.81GiB) x gpu_memory_utilization (0.79) = 109.98GiB\n",
      "INFO 10-15 03:48:24 [worker.py:295] model weights take 6.53GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 5.01GiB; the rest of the memory reserved for KV Cache is 98.27GiB.\n",
      "INFO 10-15 03:48:24 [executor_base.py:113] # cuda blocks: 115005, # CPU blocks: 7021\n",
      "INFO 10-15 03:48:24 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 56.15x\n",
      "INFO 10-15 03:48:26 [vllm_utils.py:721] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 10-15 03:48:26 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 03:48:26 [model_runner.py:1537] Graph capturing finished in 0 secs, took 0.06 GiB\n",
      "INFO 10-15 03:48:26 [vllm_utils.py:728] Unsloth: Patched vLLM v0 graph capture finished in 0 secs.\n",
      "INFO 10-15 03:48:27 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 11.51 seconds\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_attention_layernorm', 'layer_norm2', 'norm2', 'post_feedforward_layernorm', 'layer_norm1', 'pre_feedforward_layernorm', 'post_layernorm', 'input_layernorm', 'k_norm', 'norm1']\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_attention_layernorm', 'layer_norm2', 'post_feedforward_layernorm', 'layer_norm1', 'cross_attn_post_attention_layernorm', 'cross_attn_input_layernorm', 'pre_feedforward_layernorm', 'post_layernorm', 'input_layernorm', 'k_norm']\n",
      "Qwen2_5_VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='unsloth/qwen2.5-vl-7b-instruct-unsloth-bnb-4bit', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n",
      "- video_processor: Qwen2VLVideoProcessor {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"do_sample_frames\": false,\n",
      "  \"fps\": null,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_frames\": 768,\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_frames\": 4,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"num_frames\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_metadata\": false,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"size_divisor\": null,\n",
      "  \"temporal_patch_size\": 2,\n",
      "  \"video_metadata\": null,\n",
      "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
      "}\n",
      " <class 'transformers.models.qwen2_5_vl.processing_qwen2_5_vl.Qwen2_5_VLProcessor'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"001\",\n",
    "    project=\"math-vista\",\n",
    "    base_model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    ")\n",
    "backend = LocalBackend()\n",
    "await model.register(backend)\n",
    "client = model.openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e133215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model.base_model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8272a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2_5_vl.processing_qwen2_5_vl import Qwen2_5_VLProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8436dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(x for x in range(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92b4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rollout(scenario: Scenario) -> art.Trajectory:\n",
    "    image_path = f\"/tmp/{scenario['image']}\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        f.write(scenario[\"decoded_image\"][\"bytes\"])\n",
    "\n",
    "    trajectory = art.Trajectory(messages_and_choices=[], reward=0.0)\n",
    "    trajectory.messages_and_choices = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": scenario[\"question\"]\n",
    "                    + \"\\n\\nNote: Provide your answer in a LaTeX box.\",\n",
    "                },\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"file://{image_path}\"}},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        model=model.name, messages=trajectory.messages()\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    trajectory.messages_and_choices.append(choice)\n",
    "    content = choice.message.content\n",
    "    assert content is not None\n",
    "    if matches := list(re.finditer(r\"\\\\boxed\\{(.*?)\\}\", content, re.DOTALL)):\n",
    "        match = matches[-1]\n",
    "        answer = match.group(1)\n",
    "        if answer.lower() == scenario[\"answer\"].lower():\n",
    "            trajectory.reward = 1.0\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359e530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba3f8f35c7346eeab89874c8a48d61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather(val):   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6ed859147f429e80b696a716bc912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather(train):   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "\n",
    "SCENARIOS_PER_STEP = 8\n",
    "TRAJECTORY_GROUP_SIZE = 8\n",
    "start = await model.get_step()\n",
    "train_scenarios_iter = itertools.cycle(train_scenarios_iter)\n",
    "for _ in range(start * SCENARIOS_PER_STEP):\n",
    "    next(train_scenarios_iter)\n",
    "\n",
    "for i in range(start, 1000):\n",
    "    train_scenarios = [next(train_scenarios_iter) for _ in range(SCENARIOS_PER_STEP)]\n",
    "    val_trajectories, train_trajectory_groups = await asyncio.gather(\n",
    "        art.gather_trajectories(\n",
    "            (rollout(scenario) for scenario in val_scenarios),\n",
    "            pbar_desc=\"gather(val)\",\n",
    "            max_exceptions=32,\n",
    "        ),\n",
    "        art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(\n",
    "                    rollout(scenario) for _ in range(TRAJECTORY_GROUP_SIZE)\n",
    "                )\n",
    "                for scenario in train_scenarios\n",
    "            ),\n",
    "            pbar_desc=\"gather(train)\",\n",
    "            max_exceptions=32,\n",
    "        ),\n",
    "    )\n",
    "    await model.log(val_trajectories)\n",
    "    break\n",
    "    await model.train(train_trajectory_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.base_model)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad10069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "type(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58793eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2_vl.image_processing_qwen2_vl import Qwen2VLImageProcessor\n",
    "\n",
    "type(processor)\n",
    "processor.image_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b545aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in (\n",
    "    train_trajectory_groups[0].trajectories[0].messages_and_choices[0][\"content\"]\n",
    "):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb3be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend._image_processors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd6efec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151655\n",
      "{'pixel_values': tensor([[ 1.6968, -1.6171, -1.7923,  ...,  2.1459,  2.1459,  2.1459],\n",
      "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
      "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
      "        ...,\n",
      "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
      "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
      "        [ 1.9303,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459]]), 'image_grid_thw': tensor([[ 1, 12, 10]])}\n",
      "2\n",
      "30\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "A shipping company keeps track of the number of boxes in each shipment they send out. How many shipments had exactly 56 boxes? (Unit: shipments)\n",
      "\n",
      "Note: Provide your answer in a LaTeX box.<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "To answer this question, we need to interpret the stem-and-leaf plot correctly. Each row represents a range of numbers, and the 'stem' (the first digit) and 'leaf' (the second digit) together form the complete number.\n",
      "\n",
      "From the table:\n",
      "\n",
      "- The stem '5' has the leaves '0', '3', and '9'.\n",
      "  - '50' represents 50 boxes.\n",
      "  - '53' represents 53 boxes.\n",
      "  - '59' represents 59 boxes.\n",
      "\n",
      "We are looking for the number of shipments with exactly 56 boxes.\n",
      "\n",
      "Upon examining the stem '5' row, we can see that there is no stem '5' with a leaf corresponding to 6.\n",
      "\n",
      "Therefore, there are no shipments that contained exactly 56 boxes.\n",
      "\n",
      "The answer is $\\boxed{0}$.<|im_end|><|im_end|>\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrain(train_trajectory_groups)\n",
      "File \u001b[0;32m~/sky_workdir/src/art/model.py:385\u001b[0m, in \u001b[0;36mTrainableModel.train\u001b[0;34m(self, trajectory_groups, config, _config, verbose)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    371\u001b[0m     trajectory_groups: Iterable[TrajectoryGroup],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    Reinforce fine-tune the model with a batch of trajectory groups.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m            not yet part of the public API. Use at your own risk.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend()\u001b[38;5;241m.\u001b[39m_train_model(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(trajectory_groups), config, _config \u001b[38;5;129;01mor\u001b[39;00m {}, verbose\n\u001b[1;32m    387\u001b[0m     ):\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/sky_workdir/src/art/local/backend.py:431\u001b[0m, in \u001b[0;36mLocalBackend._train_model\u001b[0;34m(self, model, trajectory_groups, config, dev_config, verbose)\u001b[0m\n\u001b[1;32m    424\u001b[0m num_groups_submitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trajectory_groups)\n\u001b[1;32m    425\u001b[0m num_groups_trainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m trajectory_groups\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(trajectory\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m group)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    429\u001b[0m )\n\u001b[0;32m--> 431\u001b[0m packed_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_packed_tensors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrajectory_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43madvantage_balance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madvantage_balance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_training_without_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_training_without_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscale_rewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplot_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packed_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping tuning as there is no suitable data. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis can happen when all the trajectories in the same group \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave the same reward and thus no advantage to train on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "File \u001b[0;32m~/sky_workdir/src/art/local/backend.py:194\u001b[0m, in \u001b[0;36mLocalBackend._get_packed_tensors\u001b[0;34m(self, model, trajectory_groups, advantage_balance, allow_training_without_logprobs, scale_rewards, plot_tensors)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image_processors[model\u001b[38;5;241m.\u001b[39mbase_model] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    193\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizers[model\u001b[38;5;241m.\u001b[39mbase_model]\n\u001b[0;32m--> 194\u001b[0m tokenized_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_trajectory_groups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrajectory_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_training_without_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_rewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_image_processors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokenized_results:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sky_workdir/src/art/preprocessing/tokenize.py:75\u001b[0m, in \u001b[0;36mtokenize_trajectory_groups\u001b[0;34m(tokenizer, trajectory_groups, allow_training_without_logprobs, scale_rewards, shuffle_group_trajectories, image_processor)\u001b[0m\n\u001b[1;32m     67\u001b[0m trajectory_results: \u001b[38;5;28mlist\u001b[39m[TokenizedResult] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m history \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     69\u001b[0m     History(\n\u001b[1;32m     70\u001b[0m         messages_and_choices\u001b[38;5;241m=\u001b[39mtrajectory\u001b[38;5;241m.\u001b[39mmessages_and_choices,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m*\u001b[39mtrajectory\u001b[38;5;241m.\u001b[39madditional_histories,\n\u001b[1;32m     74\u001b[0m ]:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m:=\u001b[39m \u001b[43mtokenize_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_training_without_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     82\u001b[0m         trajectory_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     83\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28msum\u001b[39m(result\u001b[38;5;241m.\u001b[39massistant_mask) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m trajectory_results) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m     85\u001b[0m )\n",
      "File \u001b[0;32m~/sky_workdir/src/art/preprocessing/tokenize.py:267\u001b[0m, in \u001b[0;36mtokenize_trajectory\u001b[0;34m(tokenizer, image_processor, history, advantage, allow_training_without_logprobs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         assistant_mask[start:end] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m num_image_tokens\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_id) \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m token_ids))\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TokenizedResult(\n\u001b[1;32m    269\u001b[0m     advantage\u001b[38;5;241m=\u001b[39madvantage,\n\u001b[1;32m    270\u001b[0m     chat\u001b[38;5;241m=\u001b[39mchat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m    276\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "await model.train(train_trajectory_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.transformers.patches import patch_preprocess_mask_arguments\n",
    "\n",
    "patch_preprocess_mask_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trajectories = await art.gather_trajectories(\n",
    "    (rollout(scenario) for scenario in val_scenarios)\n",
    ")\n",
    "await model.log(val_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trajectories[0].messages_and_choices[-1].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee774cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "    re.finditer(\n",
    "        r\"\\\\boxed\\{(.*?)\\}\",\n",
    "        val_trajectories[0].messages_and_choices[-1].message.content,\n",
    "        re.DOTALL,\n",
    "    )\n",
    ")[-1].group(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
